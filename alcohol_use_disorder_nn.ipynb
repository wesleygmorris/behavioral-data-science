{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './data/kaggle/'\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = './data/kaggle/'\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_data = pd.read_csv(data+'kaggle_train.csv')\n",
    "test_data = pd.read_csv(data+'kaggle_test.csv')\n",
    "test_ids = test_data.pop('ID')\n",
    "train_ids = train_data.pop('ID')\n",
    "train_y = train_data.pop('TARGET')\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_y, test_size=0.2, random_state=seed)\n",
    "\n",
    "X = torch.Tensor(np.asarray(X_train))\n",
    "Y = torch.Tensor(np.asarray(y_train)).unsqueeze(1)\n",
    "training_loader = DataLoader(list(zip(X,Y)), shuffle=True, batch_size=bs)\n",
    "\n",
    "X = torch.Tensor(np.asarray(X_valid))\n",
    "Y = torch.Tensor(np.asarray(y_valid)).unsqueeze(1)\n",
    "validation_loader = DataLoader(list(zip(X,Y)), shuffle=True, batch_size=bs)\n",
    "\n",
    "print('Number of batches:', len(training_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "weight_decay = 0.01\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(17, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden1 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(60, 60)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(60,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.input(x))\n",
    "        x = self.act2(self.hidden1(x))\n",
    "        x = self.act3(self.hidden2(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "    \n",
    "model = NeuralNetwork()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "### Train for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input / label pair\n",
    "        inputs, labels = data\n",
    "        # Zero the gradients at every batch\n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions\n",
    "        outputs = model(inputs)\n",
    "        # Compute loss and gradient\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss/10\n",
    "            print(' batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      " batch 10 loss: 0.7029974818229675\n",
      " batch 20 loss: 0.6690834164619446\n",
      " batch 30 loss: 0.6972606539726257\n",
      " batch 40 loss: 0.6871707320213318\n",
      " batch 50 loss: 0.6919213950634002\n",
      " batch 60 loss: 0.6874413013458252\n",
      " batch 70 loss: 0.6917330622673035\n",
      " batch 80 loss: 0.7033853948116302\n",
      " batch 90 loss: 0.6895834982395173\n",
      " batch 100 loss: 0.6939397931098938\n",
      " batch 110 loss: 0.6829995155334473\n",
      " batch 120 loss: 0.6948353886604309\n",
      " batch 130 loss: 0.6981848835945129\n",
      " batch 140 loss: 0.6946260511875153\n",
      " batch 150 loss: 0.6896742582321167\n",
      " batch 160 loss: 0.6905553460121154\n",
      " batch 170 loss: 0.6811096727848053\n",
      " batch 180 loss: 0.6922442853450775\n",
      " batch 190 loss: 0.6981460988521576\n",
      " batch 200 loss: 0.6898232817649841\n",
      "LOSS train 0.6898232817649841 valid 0.6935492157936096\n",
      "EPOCH 2\n",
      " batch 10 loss: 0.6922598838806152\n",
      " batch 20 loss: 0.7023778975009918\n",
      " batch 30 loss: 0.6865395784378052\n",
      " batch 40 loss: 0.688350933790207\n",
      " batch 50 loss: 0.6913292288780213\n",
      " batch 60 loss: 0.6870447516441345\n",
      " batch 70 loss: 0.6807541787624359\n",
      " batch 80 loss: 0.6837050497531891\n",
      " batch 90 loss: 0.6850635409355164\n",
      " batch 100 loss: 0.6883846998214722\n",
      " batch 110 loss: 0.6853296995162964\n",
      " batch 120 loss: 0.689182436466217\n",
      " batch 130 loss: 0.6828247845172882\n",
      " batch 140 loss: 0.6944457292556763\n",
      " batch 150 loss: 0.6949676632881164\n",
      " batch 160 loss: 0.6969179391860962\n",
      " batch 170 loss: 0.6853966236114502\n",
      " batch 180 loss: 0.6867562115192414\n",
      " batch 190 loss: 0.6929083943367005\n",
      " batch 200 loss: 0.691824859380722\n",
      "LOSS train 0.691824859380722 valid 0.6904281973838806\n",
      "EPOCH 3\n",
      " batch 10 loss: 0.6902288436889649\n",
      " batch 20 loss: 0.6889566600322723\n",
      " batch 30 loss: 0.6948959767818451\n",
      " batch 40 loss: 0.6854807674884796\n",
      " batch 50 loss: 0.6846345782279968\n",
      " batch 60 loss: 0.6957970321178436\n",
      " batch 70 loss: 0.6893899142742157\n",
      " batch 80 loss: 0.6901001751422882\n",
      " batch 90 loss: 0.68664430975914\n",
      " batch 100 loss: 0.6849515795707702\n",
      " batch 110 loss: 0.6816266715526581\n",
      " batch 120 loss: 0.685248202085495\n",
      " batch 130 loss: 0.6762678563594818\n",
      " batch 140 loss: 0.6881887257099152\n",
      " batch 150 loss: 0.6814923584461212\n",
      " batch 160 loss: 0.6840375244617463\n",
      " batch 170 loss: 0.6850251972675323\n",
      " batch 180 loss: 0.6936835289001465\n",
      " batch 190 loss: 0.6880306899547577\n",
      " batch 200 loss: 0.6869777143001556\n",
      "LOSS train 0.6869777143001556 valid 0.6887959241867065\n",
      "EPOCH 4\n",
      " batch 10 loss: 0.6787994742393494\n",
      " batch 20 loss: 0.6750028610229493\n",
      " batch 30 loss: 0.6785480082035065\n",
      " batch 40 loss: 0.6748666107654572\n",
      " batch 50 loss: 0.6790934443473816\n",
      " batch 60 loss: 0.6831105172634124\n",
      " batch 70 loss: 0.6982906043529511\n",
      " batch 80 loss: 0.6876423358917236\n",
      " batch 90 loss: 0.6887975752353668\n",
      " batch 100 loss: 0.6962526202201843\n",
      " batch 110 loss: 0.6851220071315766\n",
      " batch 120 loss: 0.6778243243694305\n",
      " batch 130 loss: 0.6862688660621643\n",
      " batch 140 loss: 0.6947741091251374\n",
      " batch 150 loss: 0.687473714351654\n",
      " batch 160 loss: 0.6876319587230683\n",
      " batch 170 loss: 0.690998774766922\n",
      " batch 180 loss: 0.678408169746399\n",
      " batch 190 loss: 0.6843111336231231\n",
      " batch 200 loss: 0.6875859022140502\n",
      "LOSS train 0.6875859022140502 valid 0.6866167783737183\n",
      "EPOCH 5\n",
      " batch 10 loss: 0.6782902598381042\n",
      " batch 20 loss: 0.6870293617248535\n",
      " batch 30 loss: 0.681680428981781\n",
      " batch 40 loss: 0.685183721780777\n",
      " batch 50 loss: 0.6799085736274719\n",
      " batch 60 loss: 0.6955105423927307\n",
      " batch 70 loss: 0.6917490065097809\n",
      " batch 80 loss: 0.6796932935714721\n",
      " batch 90 loss: 0.6726450026035309\n",
      " batch 100 loss: 0.6876083016395569\n",
      " batch 110 loss: 0.6686800062656403\n",
      " batch 120 loss: 0.6855836153030396\n",
      " batch 130 loss: 0.6807076752185821\n",
      " batch 140 loss: 0.6786731541156769\n",
      " batch 150 loss: 0.6858595907688141\n",
      " batch 160 loss: 0.6788051068782807\n",
      " batch 170 loss: 0.6823361039161682\n",
      " batch 180 loss: 0.6965248465538025\n",
      " batch 190 loss: 0.6791470289230347\n",
      " batch 200 loss: 0.6776038587093354\n",
      "LOSS train 0.6776038587093354 valid 0.6841162443161011\n",
      "EPOCH 6\n",
      " batch 10 loss: 0.6731007337570191\n",
      " batch 20 loss: 0.6775390326976776\n",
      " batch 30 loss: 0.6935156404972076\n",
      " batch 40 loss: 0.6777285695075989\n",
      " batch 50 loss: 0.6740733563899994\n",
      " batch 60 loss: 0.6802519798278809\n",
      " batch 70 loss: 0.6925783693790436\n",
      " batch 80 loss: 0.6805951774120331\n",
      " batch 90 loss: 0.6832709193229676\n",
      " batch 100 loss: 0.6769895434379578\n",
      " batch 110 loss: 0.6799440264701844\n",
      " batch 120 loss: 0.680504846572876\n",
      " batch 130 loss: 0.688849675655365\n",
      " batch 140 loss: 0.6775651633739471\n",
      " batch 150 loss: 0.6861432194709778\n",
      " batch 160 loss: 0.6765445828437805\n",
      " batch 170 loss: 0.6746662020683288\n",
      " batch 180 loss: 0.6859883666038513\n",
      " batch 190 loss: 0.6738692104816437\n",
      " batch 200 loss: 0.6592188119888306\n",
      "LOSS train 0.6592188119888306 valid 0.682108461856842\n",
      "EPOCH 7\n",
      " batch 10 loss: 0.667075777053833\n",
      " batch 20 loss: 0.6816061913967133\n",
      " batch 30 loss: 0.6727458715438843\n",
      " batch 40 loss: 0.6833746075630188\n",
      " batch 50 loss: 0.6864286482334137\n",
      " batch 60 loss: 0.687660151720047\n",
      " batch 70 loss: 0.6779146373271943\n",
      " batch 80 loss: 0.6794891357421875\n",
      " batch 90 loss: 0.6932286262512207\n",
      " batch 100 loss: 0.6751348674297333\n",
      " batch 110 loss: 0.6688934385776519\n",
      " batch 120 loss: 0.6700949668884277\n",
      " batch 130 loss: 0.6926410794258118\n",
      " batch 140 loss: 0.6694191217422485\n",
      " batch 150 loss: 0.6662824988365174\n",
      " batch 160 loss: 0.670545244216919\n",
      " batch 170 loss: 0.669909542798996\n",
      " batch 180 loss: 0.6789147555828094\n",
      " batch 190 loss: 0.6747853994369507\n",
      " batch 200 loss: 0.6706845223903656\n",
      "LOSS train 0.6706845223903656 valid 0.680230975151062\n",
      "EPOCH 8\n",
      " batch 10 loss: 0.6753980100154877\n",
      " batch 20 loss: 0.6782346129417419\n",
      " batch 30 loss: 0.6835332870483398\n",
      " batch 40 loss: 0.6657081663608551\n",
      " batch 50 loss: 0.688996160030365\n",
      " batch 60 loss: 0.6581060349941253\n",
      " batch 70 loss: 0.6848158657550811\n",
      " batch 80 loss: 0.6512018740177155\n",
      " batch 90 loss: 0.6630439281463623\n",
      " batch 100 loss: 0.6695986568927765\n",
      " batch 110 loss: 0.6913681149482727\n",
      " batch 120 loss: 0.6705003678798676\n",
      " batch 130 loss: 0.6462185740470886\n",
      " batch 140 loss: 0.6849320292472839\n",
      " batch 150 loss: 0.6540181875228882\n",
      " batch 160 loss: 0.6722262501716614\n",
      " batch 170 loss: 0.6740130662918091\n",
      " batch 180 loss: 0.6799410700798034\n",
      " batch 190 loss: 0.6914479017257691\n",
      " batch 200 loss: 0.6703824579715729\n",
      "LOSS train 0.6703824579715729 valid 0.6759768128395081\n",
      "EPOCH 9\n",
      " batch 10 loss: 0.6713494241237641\n",
      " batch 20 loss: 0.65240318775177\n",
      " batch 30 loss: 0.6616056084632873\n",
      " batch 40 loss: 0.6958878457546234\n",
      " batch 50 loss: 0.6843067646026612\n",
      " batch 60 loss: 0.6890306830406189\n",
      " batch 70 loss: 0.678849595785141\n",
      " batch 80 loss: 0.6630930840969086\n",
      " batch 90 loss: 0.6733600437641144\n",
      " batch 100 loss: 0.6849290609359742\n",
      " batch 110 loss: 0.6549892604351044\n",
      " batch 120 loss: 0.6521548628807068\n",
      " batch 130 loss: 0.6754607677459716\n",
      " batch 140 loss: 0.6755526125431061\n",
      " batch 150 loss: 0.6565251588821411\n",
      " batch 160 loss: 0.6603496313095093\n",
      " batch 170 loss: 0.6681009113788605\n",
      " batch 180 loss: 0.6538507342338562\n",
      " batch 190 loss: 0.6651060461997986\n",
      " batch 200 loss: 0.6599110543727875\n",
      "LOSS train 0.6599110543727875 valid 0.6726194620132446\n",
      "EPOCH 10\n",
      " batch 10 loss: 0.6376716315746307\n",
      " batch 20 loss: 0.6716308414936065\n",
      " batch 30 loss: 0.6640630185604095\n",
      " batch 40 loss: 0.6829123616218566\n",
      " batch 50 loss: 0.6624339699745179\n",
      " batch 60 loss: 0.6543292880058289\n",
      " batch 70 loss: 0.677182936668396\n",
      " batch 80 loss: 0.6799917995929718\n",
      " batch 90 loss: 0.6754677712917327\n",
      " batch 100 loss: 0.6702599465847016\n",
      " batch 110 loss: 0.6725957930088043\n",
      " batch 120 loss: 0.65764622092247\n",
      " batch 130 loss: 0.6234705924987793\n",
      " batch 140 loss: 0.6688785016536712\n",
      " batch 150 loss: 0.6620917797088623\n",
      " batch 160 loss: 0.680997234582901\n",
      " batch 170 loss: 0.6545813858509064\n",
      " batch 180 loss: 0.6527392446994782\n",
      " batch 190 loss: 0.6647152364253998\n",
      " batch 200 loss: 0.6664542555809021\n",
      "LOSS train 0.6664542555809021 valid 0.6699278950691223\n",
      "EPOCH 11\n",
      " batch 10 loss: 0.6732537090778351\n",
      " batch 20 loss: 0.701937061548233\n",
      " batch 30 loss: 0.6569927036762238\n",
      " batch 40 loss: 0.6404462575912475\n",
      " batch 50 loss: 0.6445024669170379\n",
      " batch 60 loss: 0.658245712518692\n",
      " batch 70 loss: 0.6467597723007202\n",
      " batch 80 loss: 0.663197660446167\n",
      " batch 90 loss: 0.6887290835380554\n",
      " batch 100 loss: 0.6750787973403931\n",
      " batch 110 loss: 0.6409111320972443\n",
      " batch 120 loss: 0.643339878320694\n",
      " batch 130 loss: 0.6761591732501984\n",
      " batch 140 loss: 0.6471945166587829\n",
      " batch 150 loss: 0.6358441174030304\n",
      " batch 160 loss: 0.659005868434906\n",
      " batch 170 loss: 0.6669375240802765\n",
      " batch 180 loss: 0.6598152220249176\n",
      " batch 190 loss: 0.657150799036026\n",
      " batch 200 loss: 0.6551547467708587\n",
      "LOSS train 0.6551547467708587 valid 0.6662855744361877\n",
      "EPOCH 12\n",
      " batch 10 loss: 0.6230895161628723\n",
      " batch 20 loss: 0.6560068428516388\n",
      " batch 30 loss: 0.6838890314102173\n",
      " batch 40 loss: 0.6353327810764313\n",
      " batch 50 loss: 0.6515571713447571\n",
      " batch 60 loss: 0.6565266251564026\n",
      " batch 70 loss: 0.6498358309268951\n",
      " batch 80 loss: 0.6596219539642334\n",
      " batch 90 loss: 0.6492384135723114\n",
      " batch 100 loss: 0.6632726728916168\n",
      " batch 110 loss: 0.6473123550415039\n",
      " batch 120 loss: 0.6812494158744812\n",
      " batch 130 loss: 0.6913595557212829\n",
      " batch 140 loss: 0.6499541819095611\n",
      " batch 150 loss: 0.6471257090568543\n",
      " batch 160 loss: 0.6516781508922577\n",
      " batch 170 loss: 0.6485926032066345\n",
      " batch 180 loss: 0.6694212794303894\n",
      " batch 190 loss: 0.6259373813867569\n",
      " batch 200 loss: 0.6757570326328277\n",
      "LOSS train 0.6757570326328277 valid 0.6600290536880493\n",
      "EPOCH 13\n",
      " batch 10 loss: 0.625149530172348\n",
      " batch 20 loss: 0.6569595277309418\n",
      " batch 30 loss: 0.6455729842185974\n",
      " batch 40 loss: 0.6483429729938507\n",
      " batch 50 loss: 0.6453260600566864\n",
      " batch 60 loss: 0.6289932191371918\n",
      " batch 70 loss: 0.6306978523731231\n",
      " batch 80 loss: 0.6602037668228149\n",
      " batch 90 loss: 0.6492345988750458\n",
      " batch 100 loss: 0.665599250793457\n",
      " batch 110 loss: 0.6614868819713593\n",
      " batch 120 loss: 0.6439271032810211\n",
      " batch 130 loss: 0.6717416107654571\n",
      " batch 140 loss: 0.6239588797092438\n",
      " batch 150 loss: 0.6461187660694122\n",
      " batch 160 loss: 0.6249580889940262\n",
      " batch 170 loss: 0.6622486770153045\n",
      " batch 180 loss: 0.6553153455257416\n",
      " batch 190 loss: 0.6485368490219117\n",
      " batch 200 loss: 0.6817912757396698\n",
      "LOSS train 0.6817912757396698 valid 0.6596622467041016\n",
      "EPOCH 14\n",
      " batch 10 loss: 0.6516919434070587\n",
      " batch 20 loss: 0.6487221777439117\n",
      " batch 30 loss: 0.6709433734416962\n",
      " batch 40 loss: 0.6923506498336792\n",
      " batch 50 loss: 0.6478464603424072\n",
      " batch 60 loss: 0.6421698778867722\n",
      " batch 70 loss: 0.6426336258649826\n",
      " batch 80 loss: 0.6689611971378326\n",
      " batch 90 loss: 0.6317718386650085\n",
      " batch 100 loss: 0.6190067768096924\n",
      " batch 110 loss: 0.6374431610107422\n",
      " batch 120 loss: 0.6310112118721009\n",
      " batch 130 loss: 0.641725879907608\n",
      " batch 140 loss: 0.6291833341121673\n",
      " batch 150 loss: 0.6078097403049469\n",
      " batch 160 loss: 0.6496086418628693\n",
      " batch 170 loss: 0.6577891886234284\n",
      " batch 180 loss: 0.6404989451169968\n",
      " batch 190 loss: 0.6680911183357239\n",
      " batch 200 loss: 0.6372882425785065\n",
      "LOSS train 0.6372882425785065 valid 0.6536810994148254\n",
      "EPOCH 15\n",
      " batch 10 loss: 0.6438717842102051\n",
      " batch 20 loss: 0.6631653606891632\n",
      " batch 30 loss: 0.6827091038227081\n",
      " batch 40 loss: 0.6358676522970199\n",
      " batch 50 loss: 0.6198890745639801\n",
      " batch 60 loss: 0.6330604135990143\n",
      " batch 70 loss: 0.6560442924499512\n",
      " batch 80 loss: 0.6501252591609955\n",
      " batch 90 loss: 0.631559094786644\n",
      " batch 100 loss: 0.6740552246570587\n",
      " batch 110 loss: 0.6206925392150879\n",
      " batch 120 loss: 0.6350412786006927\n",
      " batch 130 loss: 0.6362517654895783\n",
      " batch 140 loss: 0.6952360570430756\n",
      " batch 150 loss: 0.5924028158187866\n",
      " batch 160 loss: 0.6255358338356019\n",
      " batch 170 loss: 0.6324911475181579\n",
      " batch 180 loss: 0.6281897842884063\n",
      " batch 190 loss: 0.6511778235435486\n",
      " batch 200 loss: 0.6358706414699554\n",
      "LOSS train 0.6358706414699554 valid 0.6484043598175049\n",
      "EPOCH 16\n",
      " batch 10 loss: 0.6371447503566742\n",
      " batch 20 loss: 0.6541121780872345\n",
      " batch 30 loss: 0.6581116735935211\n",
      " batch 40 loss: 0.621295040845871\n",
      " batch 50 loss: 0.6457562685012818\n",
      " batch 60 loss: 0.6259134769439697\n",
      " batch 70 loss: 0.6519592940807343\n",
      " batch 80 loss: 0.6879796326160431\n",
      " batch 90 loss: 0.6337360322475434\n",
      " batch 100 loss: 0.6623268485069275\n",
      " batch 110 loss: 0.7021696865558624\n",
      " batch 120 loss: 0.6236691951751709\n",
      " batch 130 loss: 0.621978297829628\n",
      " batch 140 loss: 0.6123068690299988\n",
      " batch 150 loss: 0.6126778513193131\n",
      " batch 160 loss: 0.6143675684928894\n",
      " batch 170 loss: 0.6587842226028442\n",
      " batch 180 loss: 0.6077253013849259\n",
      " batch 190 loss: 0.5986902385950088\n",
      " batch 200 loss: 0.635427838563919\n",
      "LOSS train 0.635427838563919 valid 0.6484451293945312\n",
      "EPOCH 17\n",
      " batch 10 loss: 0.6555719256401062\n",
      " batch 20 loss: 0.6361225545406342\n",
      " batch 30 loss: 0.6373762786388397\n",
      " batch 40 loss: 0.6613840699195862\n",
      " batch 50 loss: 0.6444307804107666\n",
      " batch 60 loss: 0.627819013595581\n",
      " batch 70 loss: 0.5998612463474273\n",
      " batch 80 loss: 0.6347988575696946\n",
      " batch 90 loss: 0.6300663828849793\n",
      " batch 100 loss: 0.6105702221393585\n",
      " batch 110 loss: 0.635512900352478\n",
      " batch 120 loss: 0.6038144886493683\n",
      " batch 130 loss: 0.653052008152008\n",
      " batch 140 loss: 0.6610174953937531\n",
      " batch 150 loss: 0.6575413942337036\n",
      " batch 160 loss: 0.6148673951625824\n",
      " batch 170 loss: 0.6310439467430115\n",
      " batch 180 loss: 0.6400340914726257\n",
      " batch 190 loss: 0.6154157757759094\n",
      " batch 200 loss: 0.6249125599861145\n",
      "LOSS train 0.6249125599861145 valid 0.6435235142707825\n",
      "EPOCH 18\n",
      " batch 10 loss: 0.623366066813469\n",
      " batch 20 loss: 0.6803792953491211\n",
      " batch 30 loss: 0.5881414353847504\n",
      " batch 40 loss: 0.6140745878219604\n",
      " batch 50 loss: 0.5886471390724182\n",
      " batch 60 loss: 0.5649294346570969\n",
      " batch 70 loss: 0.5857276111841202\n",
      " batch 80 loss: 0.5618780255317688\n",
      " batch 90 loss: 0.6979017734527588\n",
      " batch 100 loss: 0.5914558678865433\n",
      " batch 110 loss: 0.6835751891136169\n",
      " batch 120 loss: 0.6618108868598938\n",
      " batch 130 loss: 0.6720485031604767\n",
      " batch 140 loss: 0.6280405640602111\n",
      " batch 150 loss: 0.6165120363235473\n",
      " batch 160 loss: 0.6467165768146514\n",
      " batch 170 loss: 0.6520175784826279\n",
      " batch 180 loss: 0.6748398572206498\n",
      " batch 190 loss: 0.595824372768402\n",
      " batch 200 loss: 0.6350758314132691\n",
      "LOSS train 0.6350758314132691 valid 0.6433660984039307\n",
      "EPOCH 19\n",
      " batch 10 loss: 0.6218855381011963\n",
      " batch 20 loss: 0.6274018615484238\n",
      " batch 30 loss: 0.6498765528202057\n",
      " batch 40 loss: 0.5581125617027283\n",
      " batch 50 loss: 0.589153453707695\n",
      " batch 60 loss: 0.6745640486478806\n",
      " batch 70 loss: 0.6151575744152069\n",
      " batch 80 loss: 0.6215124517679215\n",
      " batch 90 loss: 0.6717267543077469\n",
      " batch 100 loss: 0.6110678523778915\n",
      " batch 110 loss: 0.687689357995987\n",
      " batch 120 loss: 0.6478958845138549\n",
      " batch 130 loss: 0.6284044265747071\n",
      " batch 140 loss: 0.5917215138673783\n",
      " batch 150 loss: 0.5897573858499527\n",
      " batch 160 loss: 0.6420293748378754\n",
      " batch 170 loss: 0.6241711735725403\n",
      " batch 180 loss: 0.6142595171928406\n",
      " batch 190 loss: 0.6418680191040039\n",
      " batch 200 loss: 0.6652975797653198\n",
      "LOSS train 0.6652975797653198 valid 0.6430842876434326\n",
      "EPOCH 20\n",
      " batch 10 loss: 0.6538565218448639\n",
      " batch 20 loss: 0.6348368018865586\n",
      " batch 30 loss: 0.6158245801925659\n",
      " batch 40 loss: 0.6596931278705597\n",
      " batch 50 loss: 0.6401395380496979\n",
      " batch 60 loss: 0.6465061455965042\n",
      " batch 70 loss: 0.6589520812034607\n",
      " batch 80 loss: 0.6099534004926681\n",
      " batch 90 loss: 0.5601384192705154\n",
      " batch 100 loss: 0.6129424303770066\n",
      " batch 110 loss: 0.6003267228603363\n",
      " batch 120 loss: 0.6514521658420562\n",
      " batch 130 loss: 0.6301469326019287\n",
      " batch 140 loss: 0.6156460165977478\n",
      " batch 150 loss: 0.6091143488883972\n",
      " batch 160 loss: 0.6087968707084656\n",
      " batch 170 loss: 0.6383366465568543\n",
      " batch 180 loss: 0.6341895520687103\n",
      " batch 190 loss: 0.6113931328058243\n",
      " batch 200 loss: 0.6158707737922668\n",
      "LOSS train 0.6158707737922668 valid 0.6423028707504272\n",
      "EPOCH 21\n",
      " batch 10 loss: 0.6687207728624344\n",
      " batch 20 loss: 0.5936060100793839\n",
      " batch 30 loss: 0.6060302227735519\n",
      " batch 40 loss: 0.6508192211389542\n",
      " batch 50 loss: 0.5849193125963211\n",
      " batch 60 loss: 0.602858817577362\n",
      " batch 70 loss: 0.5649493217468262\n",
      " batch 80 loss: 0.6438887536525726\n",
      " batch 90 loss: 0.625002846121788\n",
      " batch 100 loss: 0.6751035809516907\n",
      " batch 110 loss: 0.661964875459671\n",
      " batch 120 loss: 0.5749256670475006\n",
      " batch 130 loss: 0.62457355260849\n",
      " batch 140 loss: 0.5709774196147919\n",
      " batch 150 loss: 0.6156435221433639\n",
      " batch 160 loss: 0.6361079335212707\n",
      " batch 170 loss: 0.661160796880722\n",
      " batch 180 loss: 0.6181305944919586\n",
      " batch 190 loss: 0.6462220281362534\n",
      " batch 200 loss: 0.6651632338762283\n",
      "LOSS train 0.6651632338762283 valid 0.6429309248924255\n",
      "EPOCH 22\n",
      " batch 10 loss: 0.6643456280231476\n",
      " batch 20 loss: 0.6728993237018586\n",
      " batch 30 loss: 0.5994438886642456\n",
      " batch 40 loss: 0.6016415297985077\n",
      " batch 50 loss: 0.5974347025156022\n",
      " batch 60 loss: 0.5849262624979019\n",
      " batch 70 loss: 0.6212708055973053\n",
      " batch 80 loss: 0.5746524631977081\n",
      " batch 90 loss: 0.6380872040987015\n",
      " batch 100 loss: 0.6329373121261597\n",
      " batch 110 loss: 0.6069054454565048\n",
      " batch 120 loss: 0.6111087888479233\n",
      " batch 130 loss: 0.6129131972789764\n",
      " batch 140 loss: 0.5800662815570832\n",
      " batch 150 loss: 0.585721543431282\n",
      " batch 160 loss: 0.6245383590459823\n",
      " batch 170 loss: 0.6429254591464997\n",
      " batch 180 loss: 0.6824721753597259\n",
      " batch 190 loss: 0.6498313426971436\n",
      " batch 200 loss: 0.654458749294281\n",
      "LOSS train 0.654458749294281 valid 0.6403018236160278\n",
      "EPOCH 23\n",
      " batch 10 loss: 0.6176260381937027\n",
      " batch 20 loss: 0.6573604732751847\n",
      " batch 30 loss: 0.5831919044256211\n",
      " batch 40 loss: 0.6676765888929367\n",
      " batch 50 loss: 0.5933584868907928\n",
      " batch 60 loss: 0.6122665911912918\n",
      " batch 70 loss: 0.6273854792118072\n",
      " batch 80 loss: 0.6351358711719512\n",
      " batch 90 loss: 0.6007496833801269\n",
      " batch 100 loss: 0.636550298333168\n",
      " batch 110 loss: 0.603979280591011\n",
      " batch 120 loss: 0.6191585779190063\n",
      " batch 130 loss: 0.6193309843540191\n",
      " batch 140 loss: 0.6136098563671112\n",
      " batch 150 loss: 0.6069170564413071\n",
      " batch 160 loss: 0.6442887187004089\n",
      " batch 170 loss: 0.626834225654602\n",
      " batch 180 loss: 0.6327223062515259\n",
      " batch 190 loss: 0.5760997235774994\n",
      " batch 200 loss: 0.6559898436069489\n",
      "LOSS train 0.6559898436069489 valid 0.6427356004714966\n",
      "EPOCH 24\n",
      " batch 10 loss: 0.6594415187835694\n",
      " batch 20 loss: 0.6399925261735916\n",
      " batch 30 loss: 0.6426585137844085\n",
      " batch 40 loss: 0.666038554906845\n",
      " batch 50 loss: 0.6279930889606475\n",
      " batch 60 loss: 0.6236961662769318\n",
      " batch 70 loss: 0.6294689327478409\n",
      " batch 80 loss: 0.5962661296129227\n",
      " batch 90 loss: 0.600739473104477\n",
      " batch 100 loss: 0.5609400540590286\n",
      " batch 110 loss: 0.6536562383174896\n",
      " batch 120 loss: 0.6405205309391022\n",
      " batch 130 loss: 0.5852317303419113\n",
      " batch 140 loss: 0.624365234375\n",
      " batch 150 loss: 0.5652971297502518\n",
      " batch 160 loss: 0.5696444869041443\n",
      " batch 170 loss: 0.5962634861469269\n",
      " batch 180 loss: 0.662052208185196\n",
      " batch 190 loss: 0.6345434099435806\n",
      " batch 200 loss: 0.6067286252975463\n",
      "LOSS train 0.6067286252975463 valid 0.6430431604385376\n",
      "EPOCH 25\n",
      " batch 10 loss: 0.634498330950737\n",
      " batch 20 loss: 0.6600303888320923\n",
      " batch 30 loss: 0.6275678485631943\n",
      " batch 40 loss: 0.5641495645046234\n",
      " batch 50 loss: 0.647940781712532\n",
      " batch 60 loss: 0.5695777744054794\n",
      " batch 70 loss: 0.6454156011343002\n",
      " batch 80 loss: 0.6169140040874481\n",
      " batch 90 loss: 0.6484391152858734\n",
      " batch 100 loss: 0.6222660779953003\n",
      " batch 110 loss: 0.6108187079429627\n",
      " batch 120 loss: 0.6137613654136658\n",
      " batch 130 loss: 0.5959110885858536\n",
      " batch 140 loss: 0.6220800280570984\n",
      " batch 150 loss: 0.580684033036232\n",
      " batch 160 loss: 0.6115864157676697\n",
      " batch 170 loss: 0.5862832605838776\n",
      " batch 180 loss: 0.670105367898941\n",
      " batch 190 loss: 0.6531875014305115\n",
      " batch 200 loss: 0.5807733952999115\n",
      "LOSS train 0.5807733952999115 valid 0.641173779964447\n",
      "EPOCH 26\n",
      " batch 10 loss: 0.6047895550727844\n",
      " batch 20 loss: 0.6161830306053162\n",
      " batch 30 loss: 0.6621458798646926\n",
      " batch 40 loss: 0.5807947039604187\n",
      " batch 50 loss: 0.6084733873605728\n",
      " batch 60 loss: 0.6690041482448578\n",
      " batch 70 loss: 0.6558111429214477\n",
      " batch 80 loss: 0.624399921298027\n",
      " batch 90 loss: 0.5901046723127366\n",
      " batch 100 loss: 0.6025898069143295\n",
      " batch 110 loss: 0.6206842511892319\n",
      " batch 120 loss: 0.626587426662445\n",
      " batch 130 loss: 0.5948512375354766\n",
      " batch 140 loss: 0.5889099538326263\n",
      " batch 150 loss: 0.6720748513936996\n",
      " batch 160 loss: 0.5752084851264954\n",
      " batch 170 loss: 0.5969732940196991\n",
      " batch 180 loss: 0.6549319863319397\n",
      " batch 190 loss: 0.6071344077587127\n",
      " batch 200 loss: 0.6118483543395996\n",
      "LOSS train 0.6118483543395996 valid 0.6426605582237244\n",
      "EPOCH 27\n",
      " batch 10 loss: 0.5578324675559998\n",
      " batch 20 loss: 0.6351762175559997\n",
      " batch 30 loss: 0.6069404691457748\n",
      " batch 40 loss: 0.5955982834100724\n",
      " batch 50 loss: 0.5586912214756012\n",
      " batch 60 loss: 0.6332559764385224\n",
      " batch 70 loss: 0.6584479212760925\n",
      " batch 80 loss: 0.6637722611427307\n",
      " batch 90 loss: 0.5933689683675766\n",
      " batch 100 loss: 0.6295145153999329\n",
      " batch 110 loss: 0.6132899194955825\n",
      " batch 120 loss: 0.6077402561903\n",
      " batch 130 loss: 0.6254564553499222\n",
      " batch 140 loss: 0.6733019888401032\n",
      " batch 150 loss: 0.6205139458179474\n",
      " batch 160 loss: 0.6748197138309479\n",
      " batch 170 loss: 0.6086857110261917\n",
      " batch 180 loss: 0.6072244435548783\n",
      " batch 190 loss: 0.6145997464656829\n",
      " batch 200 loss: 0.5908948540687561\n",
      "LOSS train 0.5908948540687561 valid 0.6394314169883728\n",
      "EPOCH 28\n",
      " batch 10 loss: 0.6513640880584717\n",
      " batch 20 loss: 0.6341989576816559\n",
      " batch 30 loss: 0.534749248623848\n",
      " batch 40 loss: 0.5938812494277954\n",
      " batch 50 loss: 0.5535444915294647\n",
      " batch 60 loss: 0.6174191653728485\n",
      " batch 70 loss: 0.5891494750976562\n",
      " batch 80 loss: 0.6466074585914612\n",
      " batch 90 loss: 0.6846607863903046\n",
      " batch 100 loss: 0.6856896996498107\n",
      " batch 110 loss: 0.6246389985084534\n",
      " batch 120 loss: 0.6166128277778625\n",
      " batch 130 loss: 0.6921804904937744\n",
      " batch 140 loss: 0.6344011306762696\n",
      " batch 150 loss: 0.6444011092185974\n",
      " batch 160 loss: 0.5672340393066406\n",
      " batch 170 loss: 0.5626397788524627\n",
      " batch 180 loss: 0.6267354369163514\n",
      " batch 190 loss: 0.6474384307861328\n",
      " batch 200 loss: 0.5410260379314422\n",
      "LOSS train 0.5410260379314422 valid 0.6400817036628723\n",
      "EPOCH 29\n",
      " batch 10 loss: 0.6067400991916656\n",
      " batch 20 loss: 0.5428934186697006\n",
      " batch 30 loss: 0.6530835390090942\n",
      " batch 40 loss: 0.6383331954479218\n",
      " batch 50 loss: 0.6382646977901458\n",
      " batch 60 loss: 0.639903312921524\n",
      " batch 70 loss: 0.5598348349332809\n",
      " batch 80 loss: 0.6332901358604431\n",
      " batch 90 loss: 0.6210934907197952\n",
      " batch 100 loss: 0.5871343672275543\n",
      " batch 110 loss: 0.6218747675418854\n",
      " batch 120 loss: 0.6009567439556122\n",
      " batch 130 loss: 0.6318255454301834\n",
      " batch 140 loss: 0.5758534520864487\n",
      " batch 150 loss: 0.6682242035865784\n",
      " batch 160 loss: 0.6679806649684906\n",
      " batch 170 loss: 0.6147180736064911\n",
      " batch 180 loss: 0.6186868906021118\n",
      " batch 190 loss: 0.6269485592842102\n",
      " batch 200 loss: 0.6296828091144562\n",
      "LOSS train 0.6296828091144562 valid 0.6404425501823425\n",
      "EPOCH 30\n",
      " batch 10 loss: 0.6611204147338867\n",
      " batch 20 loss: 0.6113799929618835\n",
      " batch 30 loss: 0.5807195007801056\n",
      " batch 40 loss: 0.6379820436239243\n",
      " batch 50 loss: 0.6085808217525482\n",
      " batch 60 loss: 0.6193084836006164\n",
      " batch 70 loss: 0.6320700645446777\n",
      " batch 80 loss: 0.679049414396286\n",
      " batch 90 loss: 0.5902887254953384\n",
      " batch 100 loss: 0.597647425532341\n",
      " batch 110 loss: 0.5935510724782944\n",
      " batch 120 loss: 0.616893345117569\n",
      " batch 130 loss: 0.5911794990301132\n",
      " batch 140 loss: 0.6114101707935333\n",
      " batch 150 loss: 0.5770210713148117\n",
      " batch 160 loss: 0.6413175761699677\n",
      " batch 170 loss: 0.6667352616786957\n",
      " batch 180 loss: 0.6480890870094299\n",
      " batch 190 loss: 0.6066123336553574\n",
      " batch 200 loss: 0.5673094362020492\n",
      "LOSS train 0.5673094362020492 valid 0.638037919998169\n",
      "EPOCH 31\n",
      " batch 10 loss: 0.5817858397960662\n",
      " batch 20 loss: 0.5329586058855057\n",
      " batch 30 loss: 0.6489507645368576\n",
      " batch 40 loss: 0.5983460903167724\n",
      " batch 50 loss: 0.5952228546142578\n",
      " batch 60 loss: 0.6199683487415314\n",
      " batch 70 loss: 0.6275805234909058\n",
      " batch 80 loss: 0.5537362784147263\n",
      " batch 90 loss: 0.5891459047794342\n",
      " batch 100 loss: 0.579625004529953\n",
      " batch 110 loss: 0.6311832875013351\n",
      " batch 120 loss: 0.6787206828594208\n",
      " batch 130 loss: 0.6015977799892426\n",
      " batch 140 loss: 0.5989825189113617\n",
      " batch 150 loss: 0.6423330664634704\n",
      " batch 160 loss: 0.6501046717166901\n",
      " batch 170 loss: 0.6776120007038117\n",
      " batch 180 loss: 0.5894059896469116\n",
      " batch 190 loss: 0.6707717955112458\n",
      " batch 200 loss: 0.589018601179123\n",
      "LOSS train 0.589018601179123 valid 0.6396878361701965\n",
      "EPOCH 32\n",
      " batch 10 loss: 0.6180302828550339\n",
      " batch 20 loss: 0.6183789730072021\n",
      " batch 30 loss: 0.6099506437778472\n",
      " batch 40 loss: 0.5999028593301773\n",
      " batch 50 loss: 0.5608600825071335\n",
      " batch 60 loss: 0.5496902287006378\n",
      " batch 70 loss: 0.6218584537506103\n",
      " batch 80 loss: 0.6455789864063263\n",
      " batch 90 loss: 0.6957801163196564\n",
      " batch 100 loss: 0.5698967218399048\n",
      " batch 110 loss: 0.6379643678665161\n",
      " batch 120 loss: 0.5971854448318481\n",
      " batch 130 loss: 0.5995706975460052\n",
      " batch 140 loss: 0.5859119057655334\n",
      " batch 150 loss: 0.6412616908550263\n",
      " batch 160 loss: 0.6066529780626297\n",
      " batch 170 loss: 0.6424473524093628\n",
      " batch 180 loss: 0.6517055571079254\n",
      " batch 190 loss: 0.6151642501354218\n",
      " batch 200 loss: 0.6537036210298538\n",
      "LOSS train 0.6537036210298538 valid 0.6438798904418945\n",
      "EPOCH 33\n",
      " batch 10 loss: 0.6087630927562714\n",
      " batch 20 loss: 0.5989380449056625\n",
      " batch 30 loss: 0.6218778729438782\n",
      " batch 40 loss: 0.5871297866106033\n",
      " batch 50 loss: 0.6030929505825042\n",
      " batch 60 loss: 0.64175945520401\n",
      " batch 70 loss: 0.5606801778078079\n",
      " batch 80 loss: 0.6759521961212158\n",
      " batch 90 loss: 0.6323946088552475\n",
      " batch 100 loss: 0.6344006836414338\n",
      " batch 110 loss: 0.6044002592563629\n",
      " batch 120 loss: 0.6128153622150421\n",
      " batch 130 loss: 0.639575743675232\n",
      " batch 140 loss: 0.6016122460365295\n",
      " batch 150 loss: 0.5769427657127381\n",
      " batch 160 loss: 0.635332453250885\n",
      " batch 170 loss: 0.6216358065605163\n",
      " batch 180 loss: 0.6290024578571319\n",
      " batch 190 loss: 0.6328615456819534\n",
      " batch 200 loss: 0.5631097376346588\n",
      "LOSS train 0.5631097376346588 valid 0.6415906548500061\n",
      "EPOCH 34\n",
      " batch 10 loss: 0.651629775762558\n",
      " batch 20 loss: 0.613879269361496\n",
      " batch 30 loss: 0.6273913443088531\n",
      " batch 40 loss: 0.6679940760135651\n",
      " batch 50 loss: 0.6158404052257538\n",
      " batch 60 loss: 0.5827537029981613\n",
      " batch 70 loss: 0.5199281811714173\n",
      " batch 80 loss: 0.6278227090835571\n",
      " batch 90 loss: 0.6502671986818314\n",
      " batch 100 loss: 0.5910514742136002\n",
      " batch 110 loss: 0.6338737189769745\n",
      " batch 120 loss: 0.6035733550786972\n",
      " batch 130 loss: 0.6327391803264618\n",
      " batch 140 loss: 0.6035535305738449\n",
      " batch 150 loss: 0.5688767462968827\n",
      " batch 160 loss: 0.6216514736413956\n",
      " batch 170 loss: 0.655584841966629\n",
      " batch 180 loss: 0.6118057698011399\n",
      " batch 190 loss: 0.5992018103599548\n",
      " batch 200 loss: 0.5812318503856659\n",
      "LOSS train 0.5812318503856659 valid 0.6412370204925537\n",
      "EPOCH 35\n",
      " batch 10 loss: 0.598287308216095\n",
      " batch 20 loss: 0.5775274813175202\n",
      " batch 30 loss: 0.6349954396486283\n",
      " batch 40 loss: 0.5388037592172623\n",
      " batch 50 loss: 0.625415500998497\n",
      " batch 60 loss: 0.5802669674158096\n",
      " batch 70 loss: 0.6710068166255951\n",
      " batch 80 loss: 0.6122347563505173\n",
      " batch 90 loss: 0.6936240196228027\n",
      " batch 100 loss: 0.5273908346891403\n",
      " batch 110 loss: 0.591909846663475\n",
      " batch 120 loss: 0.6094331741333008\n",
      " batch 130 loss: 0.6391614615917206\n",
      " batch 140 loss: 0.6210386991500855\n",
      " batch 150 loss: 0.625152051448822\n",
      " batch 160 loss: 0.5823900789022446\n",
      " batch 170 loss: 0.5949429243803024\n",
      " batch 180 loss: 0.56611949801445\n",
      " batch 190 loss: 0.7241027772426605\n",
      " batch 200 loss: 0.6554381132125855\n",
      "LOSS train 0.6554381132125855 valid 0.6421025395393372\n",
      "EPOCH 36\n",
      " batch 10 loss: 0.6236090421676636\n",
      " batch 20 loss: 0.6101441085338593\n",
      " batch 30 loss: 0.612020006775856\n",
      " batch 40 loss: 0.6091470867395401\n",
      " batch 50 loss: 0.5966972172260284\n",
      " batch 60 loss: 0.5401996344327926\n",
      " batch 70 loss: 0.5787810057401657\n",
      " batch 80 loss: 0.6296288967132568\n",
      " batch 90 loss: 0.5809931695461273\n",
      " batch 100 loss: 0.6238650888204574\n",
      " batch 110 loss: 0.6195195853710175\n",
      " batch 120 loss: 0.6397249877452851\n",
      " batch 130 loss: 0.6104267835617065\n",
      " batch 140 loss: 0.6355021178722382\n",
      " batch 150 loss: 0.6285198777914047\n",
      " batch 160 loss: 0.6388198852539062\n",
      " batch 170 loss: 0.6152245551347733\n",
      " batch 180 loss: 0.5862759798765182\n",
      " batch 190 loss: 0.6413650929927825\n",
      " batch 200 loss: 0.62443887591362\n",
      "LOSS train 0.62443887591362 valid 0.6440759301185608\n",
      "EPOCH 37\n",
      " batch 10 loss: 0.5933700859546661\n",
      " batch 20 loss: 0.5703789025545121\n",
      " batch 30 loss: 0.578217551112175\n",
      " batch 40 loss: 0.6052961379289628\n",
      " batch 50 loss: 0.6086912274360656\n",
      " batch 60 loss: 0.6178430646657944\n",
      " batch 70 loss: 0.5994325965642929\n",
      " batch 80 loss: 0.6059733927249908\n",
      " batch 90 loss: 0.5855077654123306\n",
      " batch 100 loss: 0.6459536463022232\n",
      " batch 110 loss: 0.63592609167099\n",
      " batch 120 loss: 0.6002445161342621\n",
      " batch 130 loss: 0.5790428638458252\n",
      " batch 140 loss: 0.6161113679409027\n",
      " batch 150 loss: 0.57848362326622\n",
      " batch 160 loss: 0.6602021098136902\n",
      " batch 170 loss: 0.6239919841289521\n",
      " batch 180 loss: 0.6816370606422424\n",
      " batch 190 loss: 0.6412246942520141\n",
      " batch 200 loss: 0.5892451882362366\n",
      "LOSS train 0.5892451882362366 valid 0.6418670415878296\n",
      "EPOCH 38\n",
      " batch 10 loss: 0.5595433115959167\n",
      " batch 20 loss: 0.6663509547710419\n",
      " batch 30 loss: 0.6440005242824555\n",
      " batch 40 loss: 0.6423984169960022\n",
      " batch 50 loss: 0.637112894654274\n",
      " batch 60 loss: 0.6333033502101898\n",
      " batch 70 loss: 0.5888972163200379\n",
      " batch 80 loss: 0.5997101098299027\n",
      " batch 90 loss: 0.6669615805149078\n",
      " batch 100 loss: 0.6386238992214203\n",
      " batch 110 loss: 0.6113285571336746\n",
      " batch 120 loss: 0.6306863248348236\n",
      " batch 130 loss: 0.5374184519052505\n",
      " batch 140 loss: 0.6109488725662231\n",
      " batch 150 loss: 0.5647504270076752\n",
      " batch 160 loss: 0.5665338039398193\n",
      " batch 170 loss: 0.6184758156538009\n",
      " batch 180 loss: 0.5682554960250854\n",
      " batch 190 loss: 0.6209998577833176\n",
      " batch 200 loss: 0.6279372066259384\n",
      "LOSS train 0.6279372066259384 valid 0.6388075947761536\n",
      "EPOCH 39\n",
      " batch 10 loss: 0.5919034838676452\n",
      " batch 20 loss: 0.6141517192125321\n",
      " batch 30 loss: 0.6292430102825165\n",
      " batch 40 loss: 0.637686836719513\n",
      " batch 50 loss: 0.6250202298164368\n",
      " batch 60 loss: 0.57734394967556\n",
      " batch 70 loss: 0.6044184505939484\n",
      " batch 80 loss: 0.6013652205467224\n",
      " batch 90 loss: 0.5775595664978027\n",
      " batch 100 loss: 0.6248394101858139\n",
      " batch 110 loss: 0.6220593243837357\n",
      " batch 120 loss: 0.6340911835432053\n",
      " batch 130 loss: 0.6079540431499482\n",
      " batch 140 loss: 0.6272666841745377\n",
      " batch 150 loss: 0.5824005156755447\n",
      " batch 160 loss: 0.5933243423700333\n",
      " batch 170 loss: 0.6592065453529358\n",
      " batch 180 loss: 0.6417920410633087\n",
      " batch 190 loss: 0.6018521606922149\n",
      " batch 200 loss: 0.5923502653837204\n",
      "LOSS train 0.5923502653837204 valid 0.6412442326545715\n",
      "EPOCH 40\n",
      " batch 10 loss: 0.5516429305076599\n",
      " batch 20 loss: 0.6096182614564896\n",
      " batch 30 loss: 0.5849704027175904\n",
      " batch 40 loss: 0.6367086708545685\n",
      " batch 50 loss: 0.5916972815990448\n",
      " batch 60 loss: 0.6890198707580566\n",
      " batch 70 loss: 0.6613933265209198\n",
      " batch 80 loss: 0.6156045705080032\n",
      " batch 90 loss: 0.6650617390871048\n",
      " batch 100 loss: 0.5288863182067871\n",
      " batch 110 loss: 0.5641158729791641\n",
      " batch 120 loss: 0.6190733253955841\n",
      " batch 130 loss: 0.5765811115503311\n",
      " batch 140 loss: 0.5441843628883362\n",
      " batch 150 loss: 0.5773021876811981\n",
      " batch 160 loss: 0.672805905342102\n",
      " batch 170 loss: 0.649082925915718\n",
      " batch 180 loss: 0.6112835466861725\n",
      " batch 190 loss: 0.654942786693573\n",
      " batch 200 loss: 0.5985301852226257\n",
      "LOSS train 0.5985301852226257 valid 0.6402775049209595\n",
      "EPOCH 41\n",
      " batch 10 loss: 0.5822983413934708\n",
      " batch 20 loss: 0.617995697259903\n",
      " batch 30 loss: 0.6346873462200164\n",
      " batch 40 loss: 0.6144076645374298\n",
      " batch 50 loss: 0.5578396171331406\n",
      " batch 60 loss: 0.6458919256925583\n",
      " batch 70 loss: 0.5952899575233459\n",
      " batch 80 loss: 0.6416857838630676\n",
      " batch 90 loss: 0.5555539190769195\n",
      " batch 100 loss: 0.6826218396425248\n",
      " batch 110 loss: 0.571582892537117\n",
      " batch 120 loss: 0.612143623828888\n",
      " batch 130 loss: 0.6697617322206497\n",
      " batch 140 loss: 0.6134812533855438\n",
      " batch 150 loss: 0.5435361385345459\n",
      " batch 160 loss: 0.6356717467308044\n",
      " batch 170 loss: 0.5849646538496017\n",
      " batch 180 loss: 0.614441591501236\n",
      " batch 190 loss: 0.5670167863368988\n",
      " batch 200 loss: 0.6089371025562287\n",
      "LOSS train 0.6089371025562287 valid 0.6453179717063904\n",
      "EPOCH 42\n",
      " batch 10 loss: 0.6291101843118667\n",
      " batch 20 loss: 0.5966631889343261\n",
      " batch 30 loss: 0.595359155535698\n",
      " batch 40 loss: 0.6254305571317673\n",
      " batch 50 loss: 0.6487773954868317\n",
      " batch 60 loss: 0.5705632269382477\n",
      " batch 70 loss: 0.6284798949956893\n",
      " batch 80 loss: 0.6303852289915085\n",
      " batch 90 loss: 0.6607955962419509\n",
      " batch 100 loss: 0.6117572575807572\n",
      " batch 110 loss: 0.5449370354413986\n",
      " batch 120 loss: 0.6708363592624664\n",
      " batch 130 loss: 0.5751593500375748\n",
      " batch 140 loss: 0.6117313623428344\n",
      " batch 150 loss: 0.5800865679979325\n",
      " batch 160 loss: 0.6011552691459656\n",
      " batch 170 loss: 0.6490480124950408\n",
      " batch 180 loss: 0.6054339289665223\n",
      " batch 190 loss: 0.6299054175615311\n",
      " batch 200 loss: 0.5773673981428147\n",
      "LOSS train 0.5773673981428147 valid 0.6467149257659912\n",
      "EPOCH 43\n",
      " batch 10 loss: 0.6097544431686401\n",
      " batch 20 loss: 0.5740041434764862\n",
      " batch 30 loss: 0.6599036276340484\n",
      " batch 40 loss: 0.6660684287548065\n",
      " batch 50 loss: 0.6626359820365906\n",
      " batch 60 loss: 0.6878907799720764\n",
      " batch 70 loss: 0.6416374921798706\n",
      " batch 80 loss: 0.6522404283285141\n",
      " batch 90 loss: 0.5726704359054565\n",
      " batch 100 loss: 0.575212961435318\n",
      " batch 110 loss: 0.6072425127029419\n",
      " batch 120 loss: 0.5602395921945572\n",
      " batch 130 loss: 0.6319380909204483\n",
      " batch 140 loss: 0.6295433640480042\n",
      " batch 150 loss: 0.5648365795612336\n",
      " batch 160 loss: 0.5906682133674621\n",
      " batch 170 loss: 0.5934509813785553\n",
      " batch 180 loss: 0.5781077742576599\n",
      " batch 190 loss: 0.5681631565093994\n",
      " batch 200 loss: 0.5987908244132996\n",
      "LOSS train 0.5987908244132996 valid 0.6423826217651367\n",
      "EPOCH 44\n",
      " batch 10 loss: 0.6222205430269241\n",
      " batch 20 loss: 0.5695120424032212\n",
      " batch 30 loss: 0.5117052316665649\n",
      " batch 40 loss: 0.6761591851711273\n",
      " batch 50 loss: 0.5615765661001205\n",
      " batch 60 loss: 0.5663488507270813\n",
      " batch 70 loss: 0.588364765048027\n",
      " batch 80 loss: 0.7077560603618622\n",
      " batch 90 loss: 0.6377759516239166\n",
      " batch 100 loss: 0.6180749654769897\n",
      " batch 110 loss: 0.6867570877075195\n",
      " batch 120 loss: 0.5752326399087906\n",
      " batch 130 loss: 0.5804603010416031\n",
      " batch 140 loss: 0.5907839894294739\n",
      " batch 150 loss: 0.6933111906051636\n",
      " batch 160 loss: 0.5775237917900086\n",
      " batch 170 loss: 0.6511729180812835\n",
      " batch 180 loss: 0.5908843964338303\n",
      " batch 190 loss: 0.5727762669324875\n",
      " batch 200 loss: 0.6368277668952942\n",
      "LOSS train 0.6368277668952942 valid 0.64583420753479\n",
      "EPOCH 45\n",
      " batch 10 loss: 0.5793790429830551\n",
      " batch 20 loss: 0.6339864879846573\n",
      " batch 30 loss: 0.5981144636869431\n",
      " batch 40 loss: 0.5538216471672058\n",
      " batch 50 loss: 0.6450547486543655\n",
      " batch 60 loss: 0.6078090071678162\n",
      " batch 70 loss: 0.7036396205425263\n",
      " batch 80 loss: 0.6390989691019058\n",
      " batch 90 loss: 0.6155253261327743\n",
      " batch 100 loss: 0.5502542644739151\n",
      " batch 110 loss: 0.6248337149620056\n",
      " batch 120 loss: 0.6054338335990905\n",
      " batch 130 loss: 0.6001655489206315\n",
      " batch 140 loss: 0.6296950817108155\n",
      " batch 150 loss: 0.5949671924114227\n",
      " batch 160 loss: 0.6140117347240448\n",
      " batch 170 loss: 0.5979101687669754\n",
      " batch 180 loss: 0.6396259486675262\n",
      " batch 190 loss: 0.5941120535135269\n",
      " batch 200 loss: 0.546748349070549\n",
      "LOSS train 0.546748349070549 valid 0.643686830997467\n",
      "EPOCH 46\n",
      " batch 10 loss: 0.6321110099554061\n",
      " batch 20 loss: 0.6013557016849518\n",
      " batch 30 loss: 0.5721593052148819\n",
      " batch 40 loss: 0.5938360273838044\n",
      " batch 50 loss: 0.5772544741630554\n",
      " batch 60 loss: 0.6809001445770264\n",
      " batch 70 loss: 0.6082560002803803\n",
      " batch 80 loss: 0.577210682630539\n",
      " batch 90 loss: 0.6161743074655532\n",
      " batch 100 loss: 0.6738571941852569\n",
      " batch 110 loss: 0.6055392801761628\n",
      " batch 120 loss: 0.6468502461910248\n",
      " batch 130 loss: 0.6106829583644867\n",
      " batch 140 loss: 0.6158986210823059\n",
      " batch 150 loss: 0.5646725296974182\n",
      " batch 160 loss: 0.5868590325117111\n",
      " batch 170 loss: 0.5838574498891831\n",
      " batch 180 loss: 0.640700101852417\n",
      " batch 190 loss: 0.5755566477775573\n",
      " batch 200 loss: 0.6121903985738755\n",
      "LOSS train 0.6121903985738755 valid 0.6464105844497681\n",
      "EPOCH 47\n",
      " batch 10 loss: 0.5982699990272522\n",
      " batch 20 loss: 0.6377026379108429\n",
      " batch 30 loss: 0.580544289946556\n",
      " batch 40 loss: 0.6087043732404709\n",
      " batch 50 loss: 0.5049420177936554\n",
      " batch 60 loss: 0.5602981567382812\n",
      " batch 70 loss: 0.6252407431602478\n",
      " batch 80 loss: 0.6790691107511521\n",
      " batch 90 loss: 0.6458035707473755\n",
      " batch 100 loss: 0.5993483871221542\n",
      " batch 110 loss: 0.6391700446605683\n",
      " batch 120 loss: 0.6348836183547973\n",
      " batch 130 loss: 0.5568872839212418\n",
      " batch 140 loss: 0.614380595088005\n",
      " batch 150 loss: 0.6667247235774993\n",
      " batch 160 loss: 0.5745506674051285\n",
      " batch 170 loss: 0.6217316597700119\n",
      " batch 180 loss: 0.6036979138851166\n",
      " batch 190 loss: 0.6028630703687667\n",
      " batch 200 loss: 0.5963509738445282\n",
      "LOSS train 0.5963509738445282 valid 0.6399392485618591\n",
      "EPOCH 48\n",
      " batch 10 loss: 0.6187800973653793\n",
      " batch 20 loss: 0.5948397248983384\n",
      " batch 30 loss: 0.5801947861909866\n",
      " batch 40 loss: 0.6048419237136841\n",
      " batch 50 loss: 0.5890353918075562\n",
      " batch 60 loss: 0.5948022544384003\n",
      " batch 70 loss: 0.6127533823251724\n",
      " batch 80 loss: 0.6248523682355881\n",
      " batch 90 loss: 0.6458742409944535\n",
      " batch 100 loss: 0.5642402797937394\n",
      " batch 110 loss: 0.5965134412050247\n",
      " batch 120 loss: 0.6002180308103562\n",
      " batch 130 loss: 0.5676289677619935\n",
      " batch 140 loss: 0.5999077528715133\n",
      " batch 150 loss: 0.6097763538360595\n",
      " batch 160 loss: 0.619737833738327\n",
      " batch 170 loss: 0.6138289928436279\n",
      " batch 180 loss: 0.657723331451416\n",
      " batch 190 loss: 0.6735358506441116\n",
      " batch 200 loss: 0.568666809797287\n",
      "LOSS train 0.568666809797287 valid 0.6435942053794861\n",
      "EPOCH 49\n",
      " batch 10 loss: 0.5869282692670822\n",
      " batch 20 loss: 0.5561868458986282\n",
      " batch 30 loss: 0.6112726926803589\n",
      " batch 40 loss: 0.605429157614708\n",
      " batch 50 loss: 0.6322710394859314\n",
      " batch 60 loss: 0.5923655033111572\n",
      " batch 70 loss: 0.6128279954195023\n",
      " batch 80 loss: 0.6155280202627182\n",
      " batch 90 loss: 0.6456314831972122\n",
      " batch 100 loss: 0.5902822047472001\n",
      " batch 110 loss: 0.5953323066234588\n",
      " batch 120 loss: 0.5995787709951401\n",
      " batch 130 loss: 0.6394668221473694\n",
      " batch 140 loss: 0.5800662964582444\n",
      " batch 150 loss: 0.5892374187707901\n",
      " batch 160 loss: 0.5865857034921647\n",
      " batch 170 loss: 0.6225125283002854\n",
      " batch 180 loss: 0.6224746137857438\n",
      " batch 190 loss: 0.6485742390155792\n",
      " batch 200 loss: 0.565941435098648\n",
      "LOSS train 0.565941435098648 valid 0.6416918039321899\n",
      "EPOCH 50\n",
      " batch 10 loss: 0.5882228553295136\n",
      " batch 20 loss: 0.5836034715175629\n",
      " batch 30 loss: 0.6100692808628082\n",
      " batch 40 loss: 0.5591443479061127\n",
      " batch 50 loss: 0.61677106320858\n",
      " batch 60 loss: 0.5810925722122192\n",
      " batch 70 loss: 0.5887370139360428\n",
      " batch 80 loss: 0.6163134485483169\n",
      " batch 90 loss: 0.6010556757450104\n",
      " batch 100 loss: 0.6147204339504242\n",
      " batch 110 loss: 0.6561545610427857\n",
      " batch 120 loss: 0.5962117493152619\n",
      " batch 130 loss: 0.6898223102092743\n",
      " batch 140 loss: 0.7078468441963196\n",
      " batch 150 loss: 0.5716170072555542\n",
      " batch 160 loss: 0.5949075758457184\n",
      " batch 170 loss: 0.6400787770748139\n",
      " batch 180 loss: 0.576811957359314\n",
      " batch 190 loss: 0.5636562407016754\n",
      " batch 200 loss: 0.5501591473817825\n",
      "LOSS train 0.5501591473817825 valid 0.6425563097000122\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('./runs/alcoholic_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 50\n",
    "best_vloss=1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "    \n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss', { 'Training' : avg_loss, 'Validation' : avg_vloss }, epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = './bin/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best model and predict for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load('bin\\model_20240222_130438_47'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morriwg1\\AppData\\Local\\Temp\\ipykernel_19604\\3267349410.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  row_tensor = torch.Tensor(row)\n"
     ]
    }
   ],
   "source": [
    "def assign_score(row):\n",
    "    row_tensor = torch.Tensor(row)\n",
    "    with torch.no_grad():\n",
    "        pred = round(float(model(row_tensor)))\n",
    "    return pred\n",
    "\n",
    "\n",
    "preds = test_data.apply(lambda row: assign_score(row), axis=1)\n",
    "nn_submission = pd.DataFrame({'ID': test_ids, 'TARGET': preds})\n",
    "nn_submission.to_csv('./results/WESLEY_MORRIS_neural_network.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
